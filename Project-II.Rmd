---
title: "Project II ST558"
author: "Pratap Adhikari"
date: "10/9/2020"
output: rmarkdown::github_document
params:
  weekday: "monday"
  value: 1
---




```{r setup, include=FALSE}
knitr::opts_chunk$set( message = F, warning = F)
library(tidyverse)
library(knitr)
library(caret)
library(gbm)
library(rattle)
library(rpart)
library(rmarkdown)
library(ggplot2)
```

# Introduction
The project I am going to do is about the analysis of the Online News Popularity.  .The articles were said to be published by [Mashable](www.mashable.com) . The data set were downloaded from this [link](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity) . The response variable is `shares` I will be analyzing the popularity by fitting the two different tree based models: 
    + a (non ensemble) tree based model chosen using leave one out cross validation     + (LOOCV) and a boosted tree model chosen using cross-validation.



## Required packages 

The list of packages are: tidyverse, knitr, caret, gbm, rattle gbm, rmarkdown, ggplot2.

## Attribute Information:

Number of Attributes: 61 (58 predictive attributes, 2 non-predictive, 1 goal field)

### Attribute Information:
0. url: URL of the article (non-predictive)
1. timedelta: Days between the article publication and the dataset acquisition (non-predictive)
2. n_tokens_title: Number of words in the title
3. n_tokens_content: Number of words in the content
4. n_unique_tokens: Rate of unique words in the content
5. n_non_stop_words: Rate of non-stop words in the content
6. n_non_stop_unique_tokens: Rate of unique non-stop words in the content
7. num_hrefs: Number of links
8. num_self_hrefs: Number of links to other articles published by Mashable
9. num_imgs: Number of images
10. num_videos: Number of videos
11. average_token_length: Average length of the words in the content
12. num_keywords: Number of keywords in the metadata
13. data_channel_is_lifestyle: Is data channel 'Lifestyle'?
14. data_channel_is_entertainment: Is data channel 'Entertainment'?
15. data_channel_is_bus: Is data channel 'Business'?
16. data_channel_is_socmed: Is data channel 'Social Media'?
17. data_channel_is_tech: Is data channel 'Tech'?
18. data_channel_is_world: Is data channel 'World'?
19. kw_min_min: Worst keyword (min. shares)
20. kw_max_min: Worst keyword (max. shares)
21. kw_avg_min: Worst keyword (avg. shares)
22. kw_min_max: Best keyword (min. shares)
23. kw_max_max: Best keyword (max. shares)
24. kw_avg_max: Best keyword (avg. shares)
25. kw_min_avg: Avg. keyword (min. shares)
26. kw_max_avg: Avg. keyword (max. shares)
27. kw_avg_avg: Avg. keyword (avg. shares)
28. self_reference_min_shares: Min. shares of referenced articles in Mashable
29. self_reference_max_shares: Max. shares of referenced articles in Mashable
30. self_reference_avg_sharess: Avg. shares of referenced articles in Mashable
31. weekday_is_monday: Was the article published on a Monday?
32. weekday_is_tuesday: Was the article published on a Tuesday?
33. weekday_is_wednesday: Was the article published on a Wednesday?
34. weekday_is_thursday: Was the article published on a Thursday?
35. weekday_is_friday: Was the article published on a Friday?
36. weekday_is_saturday: Was the article published on a Saturday?
37. weekday_is_sunday: Was the article published on a Sunday?
38. is_weekend: Was the article published on the weekend?
39. LDA_00: Closeness to LDA topic 0
40. LDA_01: Closeness to LDA topic 1
41. LDA_02: Closeness to LDA topic 2
42. LDA_03: Closeness to LDA topic 3
43. LDA_04: Closeness to LDA topic 4
44. global_subjectivity: Text subjectivity
45. global_sentiment_polarity: Text sentiment polarity
46. global_rate_positive_words: Rate of positive words in the content
47. global_rate_negative_words: Rate of negative words in the content
48. rate_positive_words: Rate of positive words among non-neutral tokens
49. rate_negative_words: Rate of negative words among non-neutral tokens
50. avg_positive_polarity: Avg. polarity of positive words
51. min_positive_polarity: Min. polarity of positive words
52. max_positive_polarity: Max. polarity of positive words
53. avg_negative_polarity: Avg. polarity of negative words
54. min_negative_polarity: Min. polarity of negative words
55. max_negative_polarity: Max. polarity of negative words
56. title_subjectivity: Title subjectivity
57. title_sentiment_polarity: Title polarity
58. abs_title_subjectivity: Absolute subjectivity level
59. abs_title_sentiment_polarity: Absolute polarity level
60. shares: Number of shares (target)

# Read in Data set
```{r}
#Read the data
popData<- read_csv("OnlineNewsPopularity.csv")

```


## Processing

```{r}
# checking if any missing values in the raw data
anyNA(popData)

```
FALSE indicates there are no missing values. Next, to split the train and test set



# For the weekday `r params$weekday` 

```{r}
#data table

popData$num_keywords<- as.factor(popData$num_keywords)

popData<- popData %>% select(-is_weekend, -1)

#define weekdays
weekDays<- c("monday", "tuesday", "wednesday", "thursday", "friday", "saturday", "sunday")


if(params$weekday== weekDays[1]) { 
  Analysis=  popData$weekday_is_monday
  } else if (params$weekday== weekDays[2]){
    Analysis= popData$weekday_is_tuesday
  } else if (params$weekday== weekDays[3]){
    Analysis= popData$weekday_is_wednesday
  } else if (params$weekday== weekDays[4]){
    Analysis= popData$weekday_is_thursday
  } else if (params$weekday== weekDays[5]){
    Analysis= popData$weekday_is_friday
  } else if (params$weekday== weekDays[6]){
    Analysis= popData$weekday_is_saturday
  } else if (params$weekday== weekDays[7]){
    Analysis= popData$weekday_is_sunday
  } else {
    Analysis="error"
  }

finalData<- popData %>% mutate(Analysis) 
popDayData<- popData  %>% filter(Analysis == params$value) 
popDayData
```





```{r}  
# Create train and test data set.
set.seed(2001)
train<- sample(1:nrow(popDayData), size = nrow(popDayData) *0.7)
test<- dplyr::setdiff(1:nrow(popDayData), train)

popDataFinalTrain<- popDayData[train, -1]
popDataFinalTest<- popDayData[test,-1]
```

# Analysis

## Summary about the train data

```{r}
sumryData<- popDataFinalTrain %>% select(num_imgs, num_videos, n_tokens_content, global_subjectivity,  global_rate_positive_words, paste0("weekday_is_", params$weekday) , num_keywords, shares)


# summary statistics of `shares` variable
summary(sumryData$shares)

#Number of key words
kable(table(sumryData$num_keywords))

```



## Plots

### scatter plots
```{r}
#scatter plot
#number of images vs shares
ggplot(sumryData, aes(x=num_imgs, y=shares, by=num_keywords)) + geom_point(aes(color=num_keywords)) + ggtitle("Number of images vs shares") + labs(x=" Number of images")
# +  geom_smooth(method='lm', color='green')

#number of videos vs shares
ggplot(sumryData, aes(x=num_videos, y=shares, by=num_keywords)) + geom_point(aes(color=num_keywords))  + labs(x="Number of Videos", y="Shares", title= "Number of videos vs shares" )+  scale_fill_discrete(name="Number of keywords")

```

### Bar plot

```{r}
ggplot(sumryData, aes(x=num_imgs)) + geom_bar(aes(fill=num_keywords), position = "dodge") + xlab("Number of images") + scale_fill_discrete(name="Number of keywords")
ggplot(sumryData, aes(x=num_videos)) + geom_bar(aes(fill=num_keywords), position = "dodge") + xlab("Number of videos") + scale_fill_discrete(name="Number of keywords")
```

### Box plot

```{r}
ggplot(sumryData, aes(x=num_keywords, y=n_tokens_content)) + geom_boxplot() + geom_jitter(aes(color=num_keywords)) + ggtitle("Boxplot for Number of words in content") + xlab("Number of Keywords") + ylab("Number of words in the content")

```




### Histogram

```{r}

ggplot(sumryData, aes(shares, ..density.., fill=num_keywords)) + geom_histogram(bins = 100) + geom_density(col="red", lwd=1, adjust=50, alpha=0.5) + ggtitle("Histogram for Shares")

ggplot(sumryData, aes(shares, ..density..)) + geom_histogram(bins = 100) + geom_density(col="red", lwd=0.5, adjust=50) +facet_wrap(~num_keywords, ) + ggtitle("Histogram for Shares by number of words")

```



## Use of `Train` function and method=`rpart`
```{r}
popFit<- train(shares~ ., data=popDataFinalTrain[600:700, ],
             method="rpart",
             preProcess= c("center", "scale"),
             trControl= trainControl(method = "LOOCV"))
```

```{r}
plot(popFit)

#plot(popFit$finalModel); text(popFit$finalModel, pretty = 1, cex=0.8)
fancyRpartPlot(popFit$finalModel)
```


### Prediction

```{r}
predpop<- predict(popFit, newdata= dplyr::select(popDataFinalTest, -shares) )

#RMSE
treeRMSE<- sqrt(mean(predpop - popDataFinalTest$shares)^2)
```







###########################################################

## Boosted Tree method:

```{r}
#fit the model
boostFit<- gbm(shares~ ., data=popDataFinalTrain[1:50,], 
              distribution = "gaussian", 
              n.trees = 10, 
              shrinkage = 0.1, 
              interaction.depth = 4)

#prediction
boostPred<- predict(boostFit, newdata= dplyr::select(popDataFinalTest, -shares), 
                    n.trees=10)

#RMSE value
boostRMSE<- sqrt(mean(boostPred - popDataFinalTest$shares)^2 )
```

### compare RMSE values (root of test prediction error)
```{r}

#table the RMSE from both of the model fits
RMSE<- c(boost=boostRMSE, tree_based= treeRMSE)

kable(as.data.frame( RMSE), caption = "RMSE table")
```

From the above table the model having lowest value of RMSE is chosen to be appropriate to fit the data set.




